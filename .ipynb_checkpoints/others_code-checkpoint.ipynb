{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4e68e6-6717-472d-8f9f-fdc5e2670de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "# 加载plt文件（2D数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f7146a-34eb-4772-b019-7624e7526401",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = './data'  # 输入的plt文件所在目录\n",
    "output_directory = './data'  # 保存npy文件的目录\n",
    "\n",
    "\n",
    "def plt_to_npy(input_directory,output_directory):\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.plt'):\n",
    "            plt_path = os.path.join(input_directory, filename)\n",
    "            output_filename = os.path.splitext(filename)[0] + '.npy'\n",
    "            output_path = os.path.join(output_directory, output_filename)\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            # 读取plt文件数据\n",
    "            data = np.loadtxt(plt_path)\n",
    "\n",
    "            # 将数据保存为npy文件\n",
    "            np.save(output_path, data)\n",
    "    return output_path\n",
    "\n",
    "plt_data= plt_to_npy('./data','./data')\n",
    "original_height, original_width = plt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa8c39d-9885-4f5b-9d00-04c5b645a7dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the number of columns changed from 5 to 4 at row 430294; use `usecols` to select a subset and avoid this error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_path\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 调用函数\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m plt_data \u001b[38;5;241m=\u001b[39m \u001b[43mplt_to_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mplt_to_npy\u001b[1;34m(input_directory, output_directory)\u001b[0m\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(output_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 读取plt文件数据，假设标题行是前三行，使用skiprows跳过它\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 将数据保存为npy文件\u001b[39;00m\n\u001b[0;32m     15\u001b[0m np\u001b[38;5;241m.\u001b[39msave(output_path, data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ai_yellowriver_env\\lib\\site-packages\\numpy\\lib\\npyio.py:1356\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1354\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1356\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ai_yellowriver_env\\lib\\site-packages\\numpy\\lib\\npyio.py:999\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    996\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mValueError\u001b[0m: the number of columns changed from 5 to 4 at row 430294; use `usecols` to select a subset and avoid this error"
     ]
    }
   ],
   "source": [
    "# def plt_to_npy(input_directory, output_directory):\n",
    "#     for filename in os.listdir(input_directory):\n",
    "#         if filename.endswith('.plt'):\n",
    "#             plt_path = os.path.join(input_directory, filename)\n",
    "#             output_filename = os.path.splitext(filename)[0] + '.npy'\n",
    "#             output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "#             # 检查并创建输出目录\n",
    "#             os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "#             # 读取plt文件数据，假设标题行是前三行，使用skiprows跳过它\n",
    "#             data = np.loadtxt(plt_path, skiprows=3)\n",
    "\n",
    "#             # 将数据保存为npy文件\n",
    "#             np.save(output_path, data)\n",
    "#     return output_path\n",
    "\n",
    "# # 调用函数\n",
    "# plt_data = plt_to_npy('./data', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553a6e3-0d50-4c8d-a05e-f436a0c9cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义插值后的高分辨率图像的尺寸\n",
    "target_height = 2 * original_height\n",
    "target_width = 2 * original_width\n",
    "\n",
    "# 创建插值的网格\n",
    "x = np.linspace(0, original_height-1, original_height)\n",
    "y = np.linspace(0, original_width-1, original_width)\n",
    "grid_x, grid_y = np.meshgrid(x, y, indexing='ij')\n",
    "\n",
    "# 创建插值的坐标点\n",
    "new_x = np.linspace(0, original_height-1, target_height)\n",
    "new_y = np.linspace(0, original_width-1, target_width)\n",
    "new_grid_x, new_grid_y = np.meshgrid(new_x, new_y, indexing='ij')\n",
    "\n",
    "# 进行插值\n",
    "interp_data = interpolate.griddata((grid_x.flatten(), grid_y.flatten()), plt_data.flatten(),\n",
    "                                   (new_grid_x, new_grid_y), method='linear')\n",
    "\n",
    "# 可以根据需求对插值后的数据进行进一步处理和分析\n",
    "\n",
    "# 选择合适的时间步骤大小\n",
    "time_steps = 150\n",
    "height = 50\n",
    "width = 50\n",
    "\n",
    "# 数据预处理和准备\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "sequence_length = interp_data.shape[0] - time_steps   # 根据数据长度和时序长度计算\n",
    "for i in range(sequence_length):\n",
    "    input_sequences.append(interp_data[i:i+time_steps-1, :height, :width, np.newaxis])\n",
    "    output_sequences.append(interp_data[i+time_steps-1, :height, :width, np.newaxis])\n",
    "\n",
    "# 转换为数组并归一化处理（可根据具体需求调整）\n",
    "input_data = np.array(input_sequences)\n",
    "output_data = np.array(output_sequences)\n",
    "input_data = input_data / np.max(input_data)  # 归一化处理\n",
    "output_data = output_data / np.max(output_data)  # 归一化处理\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * input_data.shape[0])\n",
    "x_train, x_test = input_data[:train_size], input_data[train_size:]\n",
    "y_train, y_test = output_data[:train_size], output_data[train_size:]\n",
    "\n",
    "# 构建CNN模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(time_steps-1, height, width, 1)),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# 随机选择一个输入样本进行预测\n",
    "random_index = np.random.randint(0, x_test.shape[0])\n",
    "random_input = x_test[random_index:random_index+1]\n",
    "\n",
    "# 预测第150个时间步\n",
    "predicted_output = model.predict(random_input)\n",
    "\n",
    "# 预测下一个时刻\n",
    "next_input = np.concatenate((random_input[:, 1:, :, :, :], predicted_output), axis=1)\n",
    "next_predicted_output = model.predict(next_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
